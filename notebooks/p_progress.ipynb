{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wheelerj/miniconda3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from willireply.data import enron\n",
    "from willireply.features import features\n",
    "from willireply.features.feature_extractor import FeatureExtractor\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "import numpy as np\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Feature Extractor So Far\n",
    "\n",
    "from willireply.features import features\n",
    "\n",
    "my_common_words = ['ASAP', 'please', 'could you', 'unsubscribe', '?', '!']\n",
    "subject_common_words_feature = lambda df: features.common_words_subject(df, my_common_words)\n",
    "body_common_words_feature = lambda df: features.common_words_body(df, my_common_words)\n",
    "\n",
    "fe = FeatureExtractor(\n",
    "      subject_common_words_feature,\n",
    "      body_common_words_feature,\n",
    "      features.was_replied,\n",
    "      features.was_forwarded,\n",
    "      lambda df: np.log(1+features.number_of_ccs(df)),\n",
    "      lambda df: np.log(1+features.number_of_recipients(df)),\n",
    "      features.thread_length,\n",
    "      lambda df: np.log(1+features.words_in_body(df)),\n",
    "      features.words_in_subject\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on One User, Validate on Same User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_users = [user.stem for user in enron.ENRON_INDEX_FOLDER.iterdir() if enron.is_labeled(user.stem)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brawner-s',\n",
       " 'horton-s',\n",
       " 'neal-s',\n",
       " 'giron-d',\n",
       " 'delainey-d',\n",
       " 'ring-a',\n",
       " 'scholtes-d',\n",
       " 'davis-d',\n",
       " 'keavey-p',\n",
       " 'williams-w3',\n",
       " 'dean-c',\n",
       " 'solberg-g',\n",
       " 'merriss-s',\n",
       " 'keiser-k',\n",
       " 'stepenovitch-j',\n",
       " 'steffes-j',\n",
       " 'tycholiz-b',\n",
       " 'gang-l',\n",
       " 'forney-j',\n",
       " 'benson-r',\n",
       " 'schoolcraft-d',\n",
       " 'platter-p',\n",
       " 'harris-s',\n",
       " 'stokley-c',\n",
       " 'zufferli-j',\n",
       " 'mcconnell-m',\n",
       " 'meyers-a',\n",
       " 'perlingiere-d',\n",
       " 'shively-h',\n",
       " 'dorland-c',\n",
       " 'thomas-p',\n",
       " 'grigsby-m',\n",
       " 'fischer-m',\n",
       " 'germany-c',\n",
       " 'motley-m',\n",
       " 'hyatt-k',\n",
       " 'donohoe-t',\n",
       " 'geaccone-t',\n",
       " 'zipper-a',\n",
       " 'panus-s',\n",
       " 'pimenov-v',\n",
       " 'lewis-a',\n",
       " 'white-s',\n",
       " 'ruscitti-k',\n",
       " 'nemec-g',\n",
       " 'hodge-j',\n",
       " 'townsend-j',\n",
       " 'baughman-d',\n",
       " 'kitchen-l',\n",
       " 'bass-e',\n",
       " 'gilbertsmith-d',\n",
       " 'lucci-p']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brawner-s\n",
      "678 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.97      0.21      0.34       329\n",
      "      reply       0.03      0.80      0.06        10\n",
      "\n",
      "avg / total       0.94      0.22      0.33       339\n",
      "\n",
      "f_2 = 0.129449838188\n",
      "27\n",
      "horton-s\n",
      "1644 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.97      0.09      0.17       794\n",
      "      reply       0.03      0.93      0.07        28\n",
      "\n",
      "avg / total       0.94      0.12      0.17       822\n",
      "\n",
      "f_2 = 0.151515151515\n",
      "59\n",
      "neal-s\n",
      "2166 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.95      0.04      0.07      1002\n",
      "      reply       0.08      0.98      0.14        81\n",
      "\n",
      "avg / total       0.88      0.11      0.08      1083\n",
      "\n",
      "f_2 = 0.288742690058\n",
      "169\n",
      "giron-d\n",
      "2340 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.90      0.02      0.03      1073\n",
      "      reply       0.08      0.98      0.15        97\n",
      "\n",
      "avg / total       0.83      0.10      0.04      1170\n",
      "\n",
      "f_2 = 0.308842652796\n",
      "188\n",
      "delainey-d\n",
      "1752 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.05      0.09       783\n",
      "      reply       0.11      1.00      0.20        93\n",
      "\n",
      "avg / total       0.91      0.15      0.10       876\n",
      "\n",
      "f_2 = 0.384297520661\n",
      "198\n",
      "ring-a\n",
      "437 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.21      0.34       211\n",
      "      reply       0.04      0.88      0.08         8\n",
      "\n",
      "avg / total       0.94      0.23      0.33       219\n",
      "\n",
      "f_2 = 0.169902912621\n",
      "17\n",
      "scholtes-d\n",
      "304 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.95      0.24      0.39       147\n",
      "      reply       0.03      0.60      0.05         5\n",
      "\n",
      "avg / total       0.92      0.26      0.38       152\n",
      "\n",
      "f_2 = 0.111940298507\n",
      "12\n",
      "davis-d\n",
      "1748 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.08      0.14       802\n",
      "      reply       0.08      0.94      0.15        72\n",
      "\n",
      "avg / total       0.87      0.15      0.15       874\n",
      "\n",
      "f_2 = 0.310502283105\n",
      "126\n",
      "keavey-p\n",
      "2005 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.25      0.40       991\n",
      "      reply       0.01      0.92      0.03        12\n",
      "\n",
      "avg / total       0.98      0.26      0.39      1003\n",
      "\n",
      "f_2 = 0.068407960199\n",
      "30\n",
      "williams-w3\n",
      "2921 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.38      0.55      1397\n",
      "      reply       0.06      0.94      0.12        64\n",
      "\n",
      "avg / total       0.95      0.40      0.53      1461\n",
      "\n",
      "f_2 = 0.252525252525\n",
      "138\n",
      "dean-c\n",
      "2385 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.26      0.42      1187\n",
      "      reply       0.01      1.00      0.01         6\n",
      "\n",
      "avg / total       1.00      0.27      0.41      1193\n",
      "\n",
      "f_2 = 0.0331125827815\n",
      "16\n",
      "solberg-g\n",
      "1021 emails\n",
      "nothing\n",
      "merriss-s\n",
      "1624 emails\n",
      "nothing\n",
      "keiser-k\n",
      "748 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.06      0.12       313\n",
      "      reply       0.17      1.00      0.29        61\n",
      "\n",
      "avg / total       0.87      0.22      0.15       374\n",
      "\n",
      "f_2 = 0.510033444816\n",
      "116\n",
      "stepenovitch-j\n",
      "939 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.29      0.45       447\n",
      "      reply       0.07      1.00      0.13        23\n",
      "\n",
      "avg / total       0.95      0.33      0.43       470\n",
      "\n",
      "f_2 = 0.266203703704\n",
      "48\n",
      "steffes-j\n",
      "1798 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.93      0.02      0.04       764\n",
      "      reply       0.15      0.99      0.26       135\n",
      "\n",
      "avg / total       0.82      0.16      0.07       899\n",
      "\n",
      "f_2 = 0.470505617978\n",
      "265\n",
      "tycholiz-b\n",
      "737 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.96      0.08      0.15       327\n",
      "      reply       0.12      0.98      0.21        42\n",
      "\n",
      "avg / total       0.87      0.18      0.16       369\n",
      "\n",
      "f_2 = 0.402750491159\n",
      "86\n",
      "gang-l\n",
      "500 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.93      0.12      0.22       222\n",
      "      reply       0.12      0.93      0.21        28\n",
      "\n",
      "avg / total       0.84      0.21      0.21       250\n",
      "\n",
      "f_2 = 0.39039039039\n",
      "57\n",
      "forney-j\n",
      "297 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.95      0.14      0.24       138\n",
      "      reply       0.08      0.91      0.14        11\n",
      "\n",
      "avg / total       0.89      0.19      0.23       149\n",
      "\n",
      "f_2 = 0.28901734104\n",
      "29\n",
      "benson-r\n",
      "719 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.44      0.61       358\n",
      "      reply       0.01      1.00      0.02         2\n",
      "\n",
      "avg / total       0.99      0.44      0.61       360\n",
      "\n",
      "f_2 = 0.0473933649289\n",
      "5\n",
      "schoolcraft-d\n",
      "775 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.09      0.16       360\n",
      "      reply       0.08      1.00      0.15        28\n",
      "\n",
      "avg / total       0.93      0.15      0.16       388\n",
      "\n",
      "f_2 = 0.299145299145\n",
      "53\n",
      "platter-p\n",
      "467 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.15      0.26       226\n",
      "      reply       0.04      1.00      0.08         8\n",
      "\n",
      "avg / total       0.97      0.18      0.26       234\n",
      "\n",
      "f_2 = 0.172413793103\n",
      "18\n",
      "harris-s\n",
      "548 emails\n",
      "nothing\n",
      "stokley-c\n",
      "0 emails\n",
      "zufferli-j\n",
      "177 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.17      0.29        84\n",
      "      reply       0.07      1.00      0.12         5\n",
      "\n",
      "avg / total       0.95      0.21      0.28        89\n",
      "\n",
      "f_2 = 0.263157894737\n",
      "17\n",
      "mcconnell-m\n",
      "2685 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.06      0.11      1259\n",
      "      reply       0.07      0.99      0.12        84\n",
      "\n",
      "avg / total       0.93      0.12      0.11      1343\n",
      "\n",
      "f_2 = 0.258889582034\n",
      "167\n",
      "meyers-a\n",
      "1088 emails\n",
      "nothing\n",
      "perlingiere-d\n",
      "2426 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.89      0.02      0.04      1166\n",
      "      reply       0.04      0.94      0.07        47\n",
      "\n",
      "avg / total       0.86      0.06      0.04      1213\n",
      "\n",
      "f_2 = 0.160116448326\n",
      "108\n",
      "shively-h\n",
      "1306 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.02      0.04       625\n",
      "      reply       0.04      1.00      0.08        28\n",
      "\n",
      "avg / total       0.96      0.06      0.04       653\n",
      "\n",
      "f_2 = 0.185922974768\n",
      "70\n",
      "dorland-c\n",
      "998 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.12      0.22       484\n",
      "      reply       0.03      0.73      0.05        15\n",
      "\n",
      "avg / total       0.91      0.14      0.21       499\n",
      "\n",
      "f_2 = 0.110887096774\n",
      "33\n",
      "thomas-p\n",
      "1130 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.12      0.21       549\n",
      "      reply       0.03      0.94      0.06        16\n",
      "\n",
      "avg / total       0.96      0.14      0.21       565\n",
      "\n",
      "f_2 = 0.133214920071\n",
      "29\n",
      "grigsby-m\n",
      "1379 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.11      0.19       668\n",
      "      reply       0.04      1.00      0.07        22\n",
      "\n",
      "avg / total       0.97      0.14      0.19       690\n",
      "\n",
      "f_2 = 0.155807365439\n",
      "50\n",
      "fischer-m\n",
      "1388 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.13      0.23       677\n",
      "      reply       0.03      0.94      0.05        17\n",
      "\n",
      "avg / total       0.97      0.15      0.23       694\n",
      "\n",
      "f_2 = 0.118870728083\n",
      "30\n",
      "germany-c\n",
      "6835 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.96      0.01      0.02      2971\n",
      "      reply       0.13      1.00      0.23       447\n",
      "\n",
      "avg / total       0.86      0.14      0.05      3418\n",
      "\n",
      "f_2 = 0.430668211665\n",
      "895\n",
      "motley-m\n",
      "200 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.45      0.62        98\n",
      "      reply       0.04      1.00      0.07         2\n",
      "\n",
      "avg / total       0.98      0.46      0.61       100\n",
      "\n",
      "f_2 = 0.15625\n",
      "3\n",
      "hyatt-k\n",
      "1161 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.13      0.23       551\n",
      "      reply       0.06      1.00      0.11        30\n",
      "\n",
      "avg / total       0.95      0.18      0.22       581\n",
      "\n",
      "f_2 = 0.238473767886\n",
      "55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donohoe-t\n",
      "963 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.38      0.55       476\n",
      "      reply       0.02      1.00      0.04         6\n",
      "\n",
      "avg / total       0.99      0.39      0.54       482\n",
      "\n",
      "f_2 = 0.0920245398773\n",
      "10\n",
      "geaccone-t\n",
      "1036 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.04      0.07       467\n",
      "      reply       0.10      1.00      0.19        51\n",
      "\n",
      "avg / total       0.91      0.13      0.09       518\n",
      "\n",
      "f_2 = 0.362215909091\n",
      "102\n",
      "zipper-a\n",
      "1213 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.07      0.13       598\n",
      "      reply       0.02      1.00      0.03         9\n",
      "\n",
      "avg / total       0.99      0.08      0.12       607\n",
      "\n",
      "f_2 = 0.0746268656716\n",
      "25\n",
      "panus-s\n",
      "411 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.93      0.19      0.31       203\n",
      "      reply       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.91      0.18      0.31       206\n",
      "\n",
      "f_2 = 0.0\n",
      "7\n",
      "pimenov-v\n",
      "564 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.97      0.25      0.40       271\n",
      "      reply       0.04      0.82      0.08        11\n",
      "\n",
      "avg / total       0.94      0.27      0.39       282\n",
      "\n",
      "f_2 = 0.17578125\n",
      "26\n",
      "lewis-a\n",
      "2092 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.33      0.50      1034\n",
      "      reply       0.02      1.00      0.03        12\n",
      "\n",
      "avg / total       0.99      0.34      0.50      1046\n",
      "\n",
      "f_2 = 0.0802139037433\n",
      "19\n",
      "white-s\n",
      "2807 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.13      0.23      1330\n",
      "      reply       0.06      0.99      0.11        74\n",
      "\n",
      "avg / total       0.94      0.17      0.22      1404\n",
      "\n",
      "f_2 = 0.239030779306\n",
      "158\n",
      "ruscitti-k\n",
      "1240 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.14      0.25       589\n",
      "      reply       0.06      1.00      0.11        31\n",
      "\n",
      "avg / total       0.95      0.19      0.25       620\n",
      "\n",
      "f_2 = 0.235204855842\n",
      "70\n",
      "nemec-g\n",
      "8480 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.01      0.02      3756\n",
      "      reply       0.11      1.00      0.21       484\n",
      "\n",
      "avg / total       0.85      0.12      0.04      4240\n",
      "\n",
      "f_2 = 0.392380332139\n",
      "980\n",
      "hodge-j\n",
      "1516 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.11      0.20       739\n",
      "      reply       0.03      0.95      0.05        19\n",
      "\n",
      "avg / total       0.96      0.13      0.20       758\n",
      "\n",
      "f_2 = 0.119840213049\n",
      "38\n",
      "townsend-j\n",
      "564 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.39      0.56       280\n",
      "      reply       0.01      1.00      0.02         2\n",
      "\n",
      "avg / total       0.99      0.39      0.56       282\n",
      "\n",
      "f_2 = 0.0552486187845\n",
      "8\n",
      "baughman-d\n",
      "1687 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.21      0.35       825\n",
      "      reply       0.03      0.89      0.05        19\n",
      "\n",
      "avg / total       0.97      0.23      0.34       844\n",
      "\n",
      "f_2 = 0.114093959732\n",
      "27\n",
      "kitchen-l\n",
      "386 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.97      0.17      0.29       180\n",
      "      reply       0.07      0.92      0.14        13\n",
      "\n",
      "avg / total       0.91      0.22      0.28       193\n",
      "\n",
      "f_2 = 0.281690140845\n",
      "39\n",
      "bass-e\n",
      "4786 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.07      0.13      2153\n",
      "      reply       0.11      1.00      0.19       240\n",
      "\n",
      "avg / total       0.91      0.16      0.14      2393\n",
      "\n",
      "f_2 = 0.374531835206\n",
      "486\n",
      "gilbertsmith-d\n",
      "538 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.19      0.31       264\n",
      "      reply       0.02      1.00      0.04         5\n",
      "\n",
      "avg / total       0.98      0.20      0.31       269\n",
      "\n",
      "f_2 = 0.104166666667\n",
      "10\n",
      "lucci-p\n",
      "763 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.19      0.32       350\n",
      "      reply       0.10      0.97      0.18        32\n",
      "\n",
      "avg / total       0.91      0.26      0.31       382\n",
      "\n",
      "f_2 = 0.351473922902\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "emails_f2 = []\n",
    "\n",
    "for user in labeled_users:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "\n",
    "    # Randomly select the train and test data\n",
    "    print(user)\n",
    "    print(len(df), 'emails')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    N_training_samples = int(0.5 * len(df))\n",
    "    training_indices = np.random.choice(df.index, N_training_samples, replace=False)\n",
    "    testing_indices = set(df.index) - set(training_indices)\n",
    "\n",
    "    X_train = fe.extract(df.loc[training_indices])\n",
    "    y_train = fe.get_labels(df.loc[training_indices])\n",
    "    X_test = fe.extract(df.loc[testing_indices])\n",
    "    y_test = fe.get_labels(df.loc[testing_indices])\n",
    "\n",
    "    if sum(y_test) == 0 or sum(y_train) == 0:\n",
    "        print('nothing')\n",
    "        continue\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test) > 0\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "    f2 = fbeta_score(y_test, y_pred, 2, labels=['no reply', 'reply'], pos_label=1)\n",
    "    print('f_2 = %s' % f2)\n",
    "    print(sum(y_test) + sum(y_train))\n",
    "    \n",
    "    # Append the number of replies and the f2 score so we can take a weighted avergage\n",
    "    emails_f2.append((sum(y_test) + sum(y_train), f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 0.12944983818770225),\n",
       " (59, 0.15151515151515152),\n",
       " (169, 0.28874269005847952),\n",
       " (188, 0.30884265279583872),\n",
       " (198, 0.38429752066115702),\n",
       " (17, 0.16990291262135923),\n",
       " (12, 0.11194029850746269),\n",
       " (126, 0.31050228310502281),\n",
       " (30, 0.068407960199004969),\n",
       " (138, 0.25252525252525249),\n",
       " (16, 0.033112582781456949),\n",
       " (116, 0.51003344481605351),\n",
       " (48, 0.26620370370370366),\n",
       " (265, 0.47050561797752805),\n",
       " (86, 0.40275049115913553),\n",
       " (57, 0.39039039039039047),\n",
       " (29, 0.28901734104046245),\n",
       " (5, 0.047393364928909956),\n",
       " (53, 0.29914529914529914),\n",
       " (18, 0.17241379310344829),\n",
       " (17, 0.26315789473684209),\n",
       " (167, 0.25888958203368684),\n",
       " (108, 0.16011644832605532),\n",
       " (70, 0.18592297476759628),\n",
       " (33, 0.11088709677419357),\n",
       " (29, 0.13321492007104796),\n",
       " (50, 0.15580736543909346),\n",
       " (30, 0.11887072808320952),\n",
       " (895, 0.43066821166473546),\n",
       " (3, 0.15625),\n",
       " (55, 0.23847376788553259),\n",
       " (10, 0.092024539877300623),\n",
       " (102, 0.36221590909090912),\n",
       " (25, 0.074626865671641784),\n",
       " (7, 0.0),\n",
       " (26, 0.17578125),\n",
       " (19, 0.080213903743315509),\n",
       " (158, 0.23903077930582844),\n",
       " (70, 0.23520485584218515),\n",
       " (980, 0.39238033213936829),\n",
       " (38, 0.11984021304926766),\n",
       " (8, 0.055248618784530384),\n",
       " (27, 0.11409395973154361),\n",
       " (39, 0.28169014084507049),\n",
       " (486, 0.37453183520599254),\n",
       " (10, 0.10416666666666669),\n",
       " (70, 0.35147392290249435)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of replies, f2 score\n",
    "emails_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34004800492675774"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2s = 0\n",
    "emails = 0\n",
    "for e, f in emails_f2:\n",
    "    f2s += f*e\n",
    "    emails += e\n",
    "f2s/emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03982300884955752\n",
      "0.035888077858880776\n",
      "0.07802400738688828\n",
      "0.08034188034188035\n",
      "0.11301369863013698\n",
      "0.038901601830663615\n",
      "0.039473684210526314\n",
      "0.07208237986270023\n",
      "0.014962593516209476\n",
      "0.047244094488188976\n",
      "0.0067085953878406705\n",
      "0.002938295788442703\n",
      "0.0\n",
      "0.15508021390374332\n",
      "0.051118210862619806\n",
      "0.14738598442714126\n",
      "0.11668928086838534\n",
      "0.114\n",
      "0.09764309764309764\n",
      "0.006954102920723227\n",
      "0.06838709677419355\n",
      "0.03854389721627409\n",
      "0.0\n",
      "0.096045197740113\n",
      "0.062197392923649904\n",
      "0.0009191176470588235\n",
      "0.04451772464962902\n",
      "0.05359877488514548\n",
      "0.033066132264529056\n",
      "0.02566371681415929\n",
      "0.03625815808556925\n",
      "0.021613832853025938\n",
      "0.13094367227505485\n",
      "0.015\n",
      "0.047372954349698536\n",
      "0.010384215991692628\n",
      "0.09845559845559845\n",
      "0.020610057708161583\n",
      "0.0170316301703163\n",
      "0.04609929078014184\n",
      "0.009082217973231358\n",
      "0.056287851799073745\n",
      "0.056451612903225805\n",
      "0.11556603773584906\n",
      "0.025065963060686015\n",
      "0.014184397163120567\n",
      "0.016004742145820983\n",
      "0.10103626943005181\n",
      "0.10154617634768073\n",
      "0.01858736059479554\n",
      "0.09174311926605505\n"
     ]
    }
   ],
   "source": [
    "# Get a rough idea of how many replies the average user sends\n",
    "for user in labeled_users:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "    if len(df) == 0: continue\n",
    "    print(len(df.loc[df.did_reply==1])/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD across multiple users\n",
    "\n",
    "BIG CAVEAT!! I'm not sure this code does what it's supposed to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wheelerj/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brawner-s\n",
      "678 emails\n",
      "horton-s\n",
      "1644 emails\n",
      "neal-s\n",
      "2166 emails\n",
      "giron-d\n",
      "2340 emails\n",
      "delainey-d\n",
      "1752 emails\n",
      "ring-a\n",
      "437 emails\n",
      "scholtes-d\n",
      "304 emails\n",
      "davis-d\n",
      "1748 emails\n",
      "keavey-p\n",
      "2005 emails\n",
      "williams-w3\n",
      "2921 emails\n",
      "dean-c\n",
      "2385 emails\n",
      "solberg-g\n",
      "1021 emails\n",
      "merriss-s\n",
      "1624 emails\n",
      "nothing\n",
      "keiser-k\n",
      "748 emails\n",
      "stepenovitch-j\n",
      "939 emails\n",
      "steffes-j\n",
      "1798 emails\n",
      "tycholiz-b\n",
      "737 emails\n",
      "gang-l\n",
      "500 emails\n",
      "forney-j\n",
      "297 emails\n",
      "benson-r\n",
      "719 emails\n",
      "schoolcraft-d\n",
      "775 emails\n",
      "platter-p\n",
      "467 emails\n",
      "harris-s\n",
      "548 emails\n",
      "nothing\n",
      "stokley-c\n",
      "0 emails\n",
      "zufferli-j\n",
      "177 emails\n",
      "mcconnell-m\n",
      "2685 emails\n",
      "meyers-a\n",
      "1088 emails\n",
      "perlingiere-d\n",
      "2426 emails\n",
      "shively-h\n",
      "1306 emails\n",
      "dorland-c\n",
      "998 emails\n",
      "thomas-p\n",
      "1130 emails\n",
      "grigsby-m\n",
      "1379 emails\n",
      "fischer-m\n",
      "1388 emails\n",
      "germany-c\n",
      "6835 emails\n",
      "motley-m\n",
      "200 emails\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "emails_f2 = []\n",
    "model = SGDRegressor(warm_start=True)\n",
    "for user in labeled_users[:35]:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "\n",
    "    # Randomly select the train and test data\n",
    "    print(user)\n",
    "    print(len(df), 'emails')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    #N_training_samples = int(0.5 * len(df))\n",
    "    #training_indices = np.random.choice(df.index, N_training_samples, replace=False)\n",
    "    #testing_indices = set(df.index) - set(training_indices)\n",
    "\n",
    "    X_train = fe.extract(df) #.loc[training_indices])\n",
    "    y_train = fe.get_labels(df) #.loc[training_indices])\n",
    "    #X_test = fe.extract(df.loc[testing_indices])\n",
    "    #y_test = fe.get_labels(df.loc[testing_indices])\n",
    "\n",
    "    if sum(y_test) == 0 or sum(y_train) == 0:\n",
    "        print('nothing')\n",
    "        continue\n",
    "\n",
    "    model.partial_fit(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do all the predicting. Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyatt-k\n",
      "1161 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.96      0.43      0.59      1106\n",
      "      reply       0.05      0.64      0.10        55\n",
      "\n",
      "avg / total       0.92      0.44      0.57      1161\n",
      "\n",
      "f_2 = 0.197740112994\n",
      "58\n",
      "donohoe-t\n",
      "963 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.52      0.68       953\n",
      "      reply       0.01      0.30      0.01        10\n",
      "\n",
      "avg / total       0.98      0.52      0.68       963\n",
      "\n",
      "f_2 = 0.0302419354839\n",
      "13\n",
      "geaccone-t\n",
      "1036 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.93      0.50      0.65       934\n",
      "      reply       0.13      0.68      0.22       102\n",
      "\n",
      "avg / total       0.85      0.52      0.61      1036\n",
      "\n",
      "f_2 = 0.365853658537\n",
      "105\n",
      "zipper-a\n",
      "1213 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.47      0.64      1188\n",
      "      reply       0.02      0.56      0.04        25\n",
      "\n",
      "avg / total       0.96      0.47      0.63      1213\n",
      "\n",
      "f_2 = 0.0944669365722\n",
      "28\n",
      "panus-s\n",
      "411 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.47      0.64       404\n",
      "      reply       0.02      0.71      0.04         7\n",
      "\n",
      "avg / total       0.97      0.47      0.63       411\n",
      "\n",
      "f_2 = 0.100806451613\n",
      "10\n",
      "pimenov-v\n",
      "564 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.96      0.54      0.69       538\n",
      "      reply       0.05      0.54      0.10        26\n",
      "\n",
      "avg / total       0.92      0.54      0.67       564\n",
      "\n",
      "f_2 = 0.192307692308\n",
      "29\n",
      "lewis-a\n",
      "2092 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.23      0.37      2073\n",
      "      reply       0.01      0.74      0.02        19\n",
      "\n",
      "avg / total       0.98      0.23      0.37      2092\n",
      "\n",
      "f_2 = 0.0414937759336\n",
      "22\n",
      "white-s\n",
      "2807 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.45      0.61      2649\n",
      "      reply       0.06      0.55      0.10       158\n",
      "\n",
      "avg / total       0.89      0.45      0.58      2807\n",
      "\n",
      "f_2 = 0.198993595608\n",
      "161\n",
      "ruscitti-k\n",
      "1240 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.96      0.43      0.59      1170\n",
      "      reply       0.07      0.70      0.12        70\n",
      "\n",
      "avg / total       0.91      0.44      0.56      1240\n",
      "\n",
      "f_2 = 0.245245245245\n",
      "73\n",
      "nemec-g\n",
      "8480 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.90      0.38      0.53      7500\n",
      "      reply       0.12      0.67      0.21       980\n",
      "\n",
      "avg / total       0.81      0.41      0.49      8480\n",
      "\n",
      "f_2 = 0.352318167081\n",
      "983\n",
      "hodge-j\n",
      "1516 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.50      0.66      1478\n",
      "      reply       0.03      0.63      0.06        38\n",
      "\n",
      "avg / total       0.96      0.50      0.65      1516\n",
      "\n",
      "f_2 = 0.130576713819\n",
      "41\n",
      "townsend-j\n",
      "564 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.50      0.66       556\n",
      "      reply       0.02      0.62      0.03         8\n",
      "\n",
      "avg / total       0.98      0.50      0.66       564\n",
      "\n",
      "f_2 = 0.0793650793651\n",
      "11\n",
      "baughman-d\n",
      "1687 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.51      0.67      1660\n",
      "      reply       0.02      0.67      0.04        27\n",
      "\n",
      "avg / total       0.97      0.51      0.66      1687\n",
      "\n",
      "f_2 = 0.0959488272921\n",
      "30\n",
      "kitchen-l\n",
      "386 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.92      0.41      0.56       347\n",
      "      reply       0.11      0.67      0.19        39\n",
      "\n",
      "avg / total       0.83      0.43      0.53       386\n",
      "\n",
      "f_2 = 0.335051546392\n",
      "42\n",
      "bass-e\n",
      "4786 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.88      0.34      0.49      4300\n",
      "      reply       0.09      0.61      0.16       486\n",
      "\n",
      "avg / total       0.80      0.36      0.45      4786\n",
      "\n",
      "f_2 = 0.289499509323\n",
      "489\n",
      "gilbertsmith-d\n",
      "538 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.46      0.63       528\n",
      "      reply       0.01      0.40      0.03        10\n",
      "\n",
      "avg / total       0.96      0.46      0.62       538\n",
      "\n",
      "f_2 = 0.0609756097561\n",
      "13\n",
      "lucci-p\n",
      "763 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.42      0.58       693\n",
      "      reply       0.12      0.76      0.20        70\n",
      "\n",
      "avg / total       0.87      0.45      0.54       763\n",
      "\n",
      "f_2 = 0.359078590786\n",
      "73\n",
      "mann-k\n",
      "14455 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.78      0.33      0.46     11578\n",
      "      reply       0.19      0.61      0.29      2877\n",
      "\n",
      "avg / total       0.66      0.39      0.43     14455\n",
      "\n",
      "f_2 = 0.420669647104\n",
      "2880\n",
      "dasovich-j\n",
      "22860 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.91      0.61      0.73     20737\n",
      "      reply       0.10      0.44      0.17      2123\n",
      "\n",
      "avg / total       0.84      0.60      0.68     22860\n",
      "\n",
      "f_2 = 0.266979190487\n",
      "2126\n",
      "king-j\n",
      "439 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.39      0.56       435\n",
      "      reply       0.01      1.00      0.03         4\n",
      "\n",
      "avg / total       0.99      0.39      0.55       439\n",
      "\n",
      "f_2 = 0.0696864111498\n",
      "7\n",
      "ermis-f\n",
      "1196 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.50      0.67      1194\n",
      "      reply       0.00      0.50      0.00         2\n",
      "\n",
      "avg / total       1.00      0.50      0.67      1196\n",
      "\n",
      "f_2 = 0.00825082508251\n",
      "5\n",
      "causholli-m\n",
      "717 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.45      0.62       686\n",
      "      reply       0.06      0.81      0.12        31\n",
      "\n",
      "avg / total       0.94      0.46      0.59       717\n",
      "\n",
      "f_2 = 0.237191650854\n",
      "34\n",
      "wolfe-j\n",
      "1494 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.97      0.46      0.63      1450\n",
      "      reply       0.03      0.59      0.06        44\n",
      "\n",
      "avg / total       0.95      0.47      0.61      1494\n",
      "\n",
      "f_2 = 0.132924335378\n",
      "47\n",
      "martin-t\n",
      "859 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.49      0.64       799\n",
      "      reply       0.08      0.62      0.15        60\n",
      "\n",
      "avg / total       0.88      0.50      0.61       859\n",
      "\n",
      "f_2 = 0.269679300292\n",
      "63\n",
      "derrick-j\n",
      "1079 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.86      0.45      0.60       958\n",
      "      reply       0.09      0.44      0.15       121\n",
      "\n",
      "avg / total       0.78      0.45      0.55      1079\n",
      "\n",
      "f_2 = 0.25\n",
      "124\n",
      "griffith-j\n",
      "2780 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.31      0.48      2761\n",
      "      reply       0.01      0.63      0.01        19\n",
      "\n",
      "avg / total       0.99      0.32      0.47      2780\n",
      "\n",
      "f_2 = 0.0302419354839\n",
      "22\n",
      "arora-h\n",
      "501 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.99      0.36      0.53       486\n",
      "      reply       0.04      0.93      0.08        15\n",
      "\n",
      "avg / total       0.97      0.38      0.52       501\n",
      "\n",
      "f_2 = 0.181818181818\n",
      "18\n",
      "weldon-c\n",
      "1091 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.94      0.34      0.50      1009\n",
      "      reply       0.08      0.73      0.15        82\n",
      "\n",
      "avg / total       0.88      0.37      0.48      1091\n",
      "\n",
      "f_2 = 0.285714285714\n",
      "85\n",
      "symes-k\n",
      "7090 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.93      0.33      0.49      6486\n",
      "      reply       0.09      0.71      0.16       604\n",
      "\n",
      "avg / total       0.85      0.37      0.46      7090\n",
      "\n",
      "f_2 = 0.300390298299\n",
      "607\n",
      "swerzbin-m\n",
      "282 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.98      0.47      0.64       272\n",
      "      reply       0.05      0.80      0.10        10\n",
      "\n",
      "avg / total       0.95      0.49      0.62       282\n",
      "\n",
      "f_2 = 0.20942408377\n",
      "13\n",
      "south-s\n",
      "206 emails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wheelerj/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/wheelerj/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       1.00      0.49      0.66       206\n",
      "      reply       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.49      0.66       206\n",
      "\n",
      "f_2 = 0.0\n",
      "3\n",
      "kaminski-v\n",
      "18031 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.83      0.31      0.45     15395\n",
      "      reply       0.14      0.64      0.23      2636\n",
      "\n",
      "avg / total       0.73      0.36      0.42     18031\n",
      "\n",
      "f_2 = 0.369778439443\n",
      "2639\n",
      "heard-m\n",
      "594 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.91      0.53      0.67       517\n",
      "      reply       0.17      0.64      0.27        77\n",
      "\n",
      "avg / total       0.81      0.55      0.62       594\n",
      "\n",
      "f_2 = 0.409015025042\n",
      "80\n",
      "blair-l\n",
      "2397 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.89      0.36      0.51      2242\n",
      "      reply       0.04      0.39      0.07       155\n",
      "\n",
      "avg / total       0.84      0.36      0.48      2397\n",
      "\n",
      "f_2 = 0.143596986817\n",
      "158\n",
      "holst-k\n",
      "427 emails\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.96      0.37      0.53       416\n",
      "      reply       0.02      0.45      0.04        11\n",
      "\n",
      "avg / total       0.94      0.37      0.52       427\n",
      "\n",
      "f_2 = 0.08038585209\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "emails_f2 = []    \n",
    "\n",
    "for user in labeled_users[35:]:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "\n",
    "    # Randomly select the train and test data\n",
    "    print(user)\n",
    "    print(len(df), 'emails')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "\n",
    "    X_test = fe.extract(df)# .loc[testing_indices])\n",
    "    y_test = fe.get_labels(df) # .loc[testing_indices])\n",
    "    y_pred = model.predict(X_test) > 0\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "    f2 = fbeta_score(y_test, y_pred, 2, labels=['no reply', 'reply'], pos_label=1)\n",
    "    print('f_2 = %s' % f2)\n",
    "    print(sum(y_test) + sum(y_train))\n",
    "    emails_f2.append((sum(y_test), f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33774663006978412"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2s = 0\n",
    "emails = 0\n",
    "for e, f in emails_f2:\n",
    "    f2s += f*e\n",
    "    emails += e\n",
    "f2s/emails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
