{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from willireply.data import enron\n",
    "from willireply.features import features\n",
    "from willireply.features.feature_extractor import FeatureExtractor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.neural_network import MLPRegressor,MLPClassifier\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "import numpy as np\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Feature Extractor So Far\n",
    "\n",
    "from willireply.features import features\n",
    "\n",
    "my_common_words = ['ASAP', 'please', 'could you', 'unsubscribe', '?', '!']\n",
    "subject_common_words_feature = lambda df: features.common_words_subject(df, my_common_words)\n",
    "body_common_words_feature = lambda df: features.common_words_body(df, my_common_words)\n",
    "\n",
    "fe = FeatureExtractor(\n",
    "      subject_common_words_feature,\n",
    "      body_common_words_feature,\n",
    "      features.was_replied,\n",
    "      features.was_forwarded,\n",
    "      lambda df: np.log(1+features.number_of_ccs(df)),\n",
    "      lambda df: np.log(1+features.number_of_recipients(df)),\n",
    "      features.thread_length,\n",
    "      lambda df: np.log(1+features.words_in_body(df)),\n",
    "      features.words_in_subject\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on One User, Validate on Same User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_users = [user.stem for user in enron.ENRON_INDEX_FOLDER.iterdir() if enron.is_labeled(user.stem)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ring-a',\n 'stepenovitch-j',\n 'shively-h',\n 'hodge-j',\n 'baughman-d',\n 'donoho-l',\n 'may-l',\n 'sanders-r',\n 'crandell-s',\n 'hayslett-r']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ring-a\n437 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.98      0.98      0.98        86\n      reply       0.00      0.00      0.00         2\n\navg / total       0.95      0.95      0.95        88\n\nf_2 = 0.0\n17\nstepenovitch-j\n939 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.96      0.97      0.96       174\n      reply       0.54      0.50      0.52        14\n\navg / total       0.93      0.93      0.93       188\n\nf_2 = 0.507246376812\n48\nshively-h\n1306 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.94      0.97      0.96       245\n      reply       0.22      0.12      0.15        17\n\navg / total       0.89      0.92      0.90       262\n\nf_2 = 0.12987012987\n70\nhodge-j\n1516 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.96      0.99      0.98       293\n      reply       0.00      0.00      0.00        11\n\navg / total       0.93      0.95      0.94       304\n\nf_2 = 0.0\n38\nbaughman-d\n1687 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.99      0.99      0.99       335\n      reply       0.00      0.00      0.00         3\n\navg / total       0.98      0.98      0.98       338\n\nf_2 = 0.0\n27\ndonoho-l\n317 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.95      0.97      0.96        61\n      reply       0.00      0.00      0.00         3\n\navg / total       0.91      0.92      0.91        64\n\nf_2 = 0.0\n14\nmay-l\n1492 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.98      0.99      0.99       289\n      reply       0.71      0.50      0.59        10\n\navg / total       0.97      0.98      0.97       299\n\nf_2 = 0.531914893617\n38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanders-r\n5218 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.95      0.96      0.96       951\n      reply       0.54      0.54      0.54        93\n\navg / total       0.92      0.92      0.92      1044\n\nf_2 = 0.538793103448\n481\ncrandell-s\n342 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.90      0.89      0.90        63\n      reply       0.00      0.00      0.00         6\n\navg / total       0.82      0.81      0.82        69\n\nf_2 = 0.0\n26\nhayslett-r\n1655 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.94      0.98      0.96       299\n      reply       0.70      0.44      0.54        32\n\navg / total       0.92      0.93      0.92       331\n\nf_2 = 0.472972972973\n164\n"
     ]
    }
   ],
   "source": [
    "emails_f2 = []\n",
    "\n",
    "for user in labeled_users:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "\n",
    "    # Randomly select the train and test data\n",
    "    print(user)\n",
    "    print(len(df), 'emails')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    N_training_samples = int(0.8 * len(df))\n",
    "    training_indices = np.random.choice(df.index, N_training_samples, replace=False)\n",
    "    testing_indices = set(df.index) - set(training_indices)\n",
    "\n",
    "    X_train = fe.extract(df.loc[training_indices])\n",
    "    y_train = fe.get_labels(df.loc[training_indices])\n",
    "    X_test = fe.extract(df.loc[testing_indices])\n",
    "    y_test = fe.get_labels(df.loc[testing_indices])\n",
    "\n",
    "    if sum(y_test) == 0 or sum(y_train) == 0:\n",
    "        print('nothing')\n",
    "        continue\n",
    "\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test) > 0\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "    f2 = fbeta_score(y_test, y_pred, 2, labels=['no reply', 'reply'], pos_label=1)\n",
    "    print('f_2 = %s' % f2)\n",
    "    print(sum(y_test) + sum(y_train))\n",
    "    \n",
    "    # Append the number of replies and the f2 score so we can take a weighted avergage\n",
    "    emails_f2.append((sum(y_test) + sum(y_train), f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, 0.0),\n (48, 0.50724637681159412),\n (70, 0.12987012987012989),\n (38, 0.0),\n (27, 0.0),\n (14, 0.0),\n (38, 0.53191489361702127),\n (481, 0.53879310344827591),\n (26, 0.0),\n (164, 0.47297297297297297)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of replies, f2 score\n",
    "emails_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49676923038466025"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2s = 0\n",
    "emails = 0\n",
    "for e, f in emails_f2:\n",
    "    f2s += f*e\n",
    "    emails += e\n",
    "f2s/emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038901601830663615\n0.051118210862619806\n0.05359877488514548\n0.025065963060686015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016004742145820983\n0.04416403785488959\n0.02546916890080429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09218091222690686\n0.07602339181286549\n0.09909365558912386\n"
     ]
    }
   ],
   "source": [
    "# Get a rough idea of how many replies the average user sends\n",
    "for user in labeled_users:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "    if len(df) == 0: continue\n",
    "    print(len(df.loc[df.did_reply==1])/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD across multiple users\n",
    "\n",
    "BIG CAVEAT!! I'm not sure this code does what it's supposed to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ring-a\n437 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stepenovitch-j\n939 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shively-h\n1306 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hodge-j\n1516 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baughman-d\n1687 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donoho-l\n317 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may-l\n1492 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanders-r\n5218 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crandell-s\n342 emails\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hayslett-r\n1655 emails\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "emails_f2 = []\n",
    "model = DecisionTreeClassifier()#warm_start=True)\n",
    "for user in labeled_users[:35]:\n",
    "    df = enron.get_dataframe(user, received_only=True)\n",
    "\n",
    "    # Randomly select the train and test data\n",
    "    print(user)\n",
    "    print(len(df), 'emails')\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    #N_training_samples = int(0.5 * len(df))\n",
    "    #training_indices = np.random.choice(df.index, N_training_samples, replace=False)\n",
    "    #testing_indices = set(df.index) - set(training_indices)\n",
    "\n",
    "    X_train = fe.extract(df) #.loc[training_indices])\n",
    "    y_train = fe.get_labels(df) #.loc[training_indices])\n",
    "    #X_test = fe.extract(df.loc[testing_indices])\n",
    "    #y_test = fe.get_labels(df.loc[testing_indices])\n",
    "\n",
    "    if sum(y_test) == 0 or sum(y_train) == 0:\n",
    "        print('nothing')\n",
    "        continue\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
