{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier,LogisticRegression, RandomizedLogisticRegression\n",
    "from sklearn.metrics import classification_report, fbeta_score, accuracy_score\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data were run using the `make_dataset.py` script in `../src/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data/raw/data_with_fw.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM emails WHERE folder='received'\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3106553276638319"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times did we respond, out of how many rows there are total?\n",
    "len(df.query('did_reply==1')) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['did_reply'].values\n",
    "true_indices = np.where(y_true)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my friends went through all of the emails, and marked which ones she thought I was likely to respond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_indices = [11, 31, 39, 51, 63, 68, 74, 78, 120, 122, 131, 133, 134, 138, 145, 159, 179]\n",
    "y_oracle = np.zeros_like(y_true)\n",
    "for i in oracle_indices:\n",
    "    y_oracle[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She was correct for four of the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{63, 131, 133, 179}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(true_indices).intersection(oracle_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.87      0.92      0.90       172\n",
      "      reply       0.24      0.14      0.18        28\n",
      "\n",
      "avg / total       0.78      0.81      0.80       200\n",
      "\n",
      "f_2 = 0.15503875968992248\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_oracle, target_names=['no reply', 'reply']))\n",
    "print('f_2 = %s' % fbeta_score(y_true, y_oracle, 2, labels=['no reply', 'reply'], pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a machine doe better? Let's cut the dataset into a training slice (first 100 samples) and a validation slice (last 100 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice = slice(0, 1000)\n",
    "test_slice = slice(500, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a feature vector based on whether the sender is somebody who we have replied to in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites = list(Counter(df.iloc[train_slice].query('did_reply == 1')['sender'].values).keys())\n",
    "len(favorites)\n",
    "\n",
    "def featureExtractor(row):\n",
    "    favorites_vector = [favorite in row['sender'] for favorite in favorites]\n",
    "    return favorites_vector\n",
    "\n",
    "def label(row):\n",
    "    return row['did_reply']\n",
    "\n",
    "X = np.array([featureExtractor(row) for idx, row in df.iterrows()])\n",
    "y_true = np.array([label(row) for idx, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning magic happens here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X[train_slice], y_true[train_slice])\n",
    "y_pred = model.predict(X[test_slice]) > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n\n   no reply       0.77      0.97      0.85       344\n      reply       0.82      0.35      0.49       156\n\navg / total       0.78      0.77      0.74       500\n\nf_2 = 0.397973950796\n0.774\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true[test_slice], y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "print('f_2 = %s' % fbeta_score(y_true[test_slice], y_pred, 2, labels=['no reply', 'reply'], pos_label=1))\n",
    "print(accuracy_score(y_true[test_slice], y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Favorites extractor from before\n",
    "def favoritesExtractor(row):\n",
    "    favorites_vector = [favorite in row['sender'] for favorite in favorites]\n",
    "    return favorites_vector\n",
    "\n",
    "#Perform rudimentary sentiment analysis on entire message body\n",
    "def sentimentBodyExtractor(row):\n",
    "    if row['subject'] is not None:\n",
    "        \n",
    "        polarity, subjectivity = TextBlob(row['body']).sentiment\n",
    "    else:\n",
    "        polarity, subjectivity = 0,0\n",
    "        \n",
    "    return [polarity,subjectivity]\n",
    "\n",
    "#Perform rudimentary sentiment analysis on entire message subject\n",
    "def sentimentSubjectExtractor(row):\n",
    "    if row['subject'] is not None:\n",
    "        \n",
    "        polarity, subjectivity = TextBlob(row['subject']).sentiment\n",
    "    else:\n",
    "        polarity, subjectivity = 0,0\n",
    "        \n",
    "    return [polarity,subjectivity]\n",
    "\n",
    "#Count occurences of keywords\n",
    "def keywordExtractor(row):\n",
    "    keywords = [\"please\", \"response\", \"request\", \"reply\", \"when\", \"thanks\", \"?\", ]\n",
    "    score = 0\n",
    "    text = TextBlob(row['body'])\n",
    "    for word in text.words:\n",
    "        if str(word).lower() in keywords:\n",
    "            score += 1\n",
    "    return [score]\n",
    "\n",
    "\n",
    "#Determine whether there exists more than one recipient\n",
    "def recipientsExtractor(row):\n",
    "    feature_vector = [row['multiple_recipients'] == 1]\n",
    "    return feature_vector\n",
    "\n",
    "#length of body\n",
    "def lengthExtractor(row):\n",
    "    return [len(row['body'])]\n",
    "\n",
    "#Determine if \"Re: \" appears in subject line\n",
    "def reExtractor(row):\n",
    "    re_vector = [row['re_in_subject'] == 1]\n",
    "    return  re_vector\n",
    "\n",
    "#Determine if \"Fwd: \" appears in subject line\n",
    "def fwdExtractor(row):\n",
    "    fw_vector = [row['fw_in_subject'] == 1]\n",
    "    return fw_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run more machine learning magic, using additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n500\nLinear Regressor\n             precision    recall  f1-score   support\n\n   no reply       0.82      0.90      0.86       344\n      reply       0.72      0.57      0.64       156\n\navg / total       0.79      0.80      0.79       500\n\nf_2 = 0.594919786096\n0.796\n"
     ]
    }
   ],
   "source": [
    "X = np.array([favoritesExtractor(row)\n",
    "              + lengthExtractor(row)\n",
    "              + recipientsExtractor(row)\n",
    "              + reExtractor(row)\n",
    "              + fwdExtractor(row)\n",
    "              + sentimentBodyExtractor(row)\n",
    "              + sentimentSubjectExtractor(row)\n",
    "              + keywordExtractor(row)\n",
    "              for idx, row in df.iterrows()])\n",
    "y_true = np.array([label(row) for idx, row in df.iterrows()])\n",
    "print(len(X[test_slice]))\n",
    "print(len(y_true[test_slice]))\n",
    "print(\"Linear Regressor\")\n",
    "model = LinearRegression()\n",
    "model.fit(X[train_slice], y_true[train_slice])\n",
    "y_pred = model.predict(X[test_slice]) > .5\n",
    "print(classification_report(y_true[test_slice], y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "print('f_2 = %s' % fbeta_score(y_true[test_slice], y_pred, 2, labels=['no reply', 'reply'], pos_label=1))\n",
    "print(accuracy_score(y_true[test_slice], y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple features, performance is about the same. Let's use a different classifier that is more suitable to the number of features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is degraded when using a new classifier. Probably for reasons of insufficient data. Lets increase the slice sizes (the more_features branch has modified the download script to download more emails):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice = slice(0, 1500)\n",
    "test_slice = slice(1500, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier\n             precision    recall  f1-score   support\n\n   no reply       0.97      0.99      0.98       344\n      reply       0.98      0.93      0.95       156\n\navg / total       0.97      0.97      0.97       500\n\nf_2 = 0.939119170984\n0.972\n"
     ]
    }
   ],
   "source": [
    "X = np.array([favoritesExtractor(row)\n",
    "              + lengthExtractor(row)\n",
    "              + recipientsExtractor(row)\n",
    "              + reExtractor(row)\n",
    "              + fwdExtractor(row)\n",
    "              + sentimentBodyExtractor(row)\n",
    "              + sentimentSubjectExtractor(row)\n",
    "              + keywordExtractor(row)\n",
    "              for idx, row in df.iterrows()])\n",
    "y_true = np.array([label(row) for idx, row in df.iterrows()])\n",
    "model = RandomForestClassifier()  \n",
    "\n",
    "print(\"MLPClassifier\")\n",
    "#model = MLPClassifier(solver='relu', learning_rate='adaptive', hidden_layer_sizes=(100,100,), verbose=True, early_stopping=False, max_iter=2000)\n",
    "model.fit(X[train_slice], y_true[train_slice])\n",
    "#print(\"Features sorted by their score:\")\n",
    "#print(sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), X[train_slice]), reverse=True))\n",
    "y_pred = model.predict(X[test_slice]) > .5\n",
    "print(classification_report(y_true[test_slice], y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "print('f_2 = %s' % fbeta_score(y_true[test_slice], y_pred, 2, labels=['no reply', 'reply'], pos_label=1))\n",
    "print(accuracy_score(y_true[test_slice], y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithm seems to work much better than the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
