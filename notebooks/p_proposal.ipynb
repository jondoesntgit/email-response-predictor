{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data were run using the `make_dataset.py` script in `../src/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"../data/raw/p-proposal.sqlite3\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM emails WHERE folder='received'\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times did we respond, out of how many rows there are total?\n",
    "len(df.query('did_reply==1')) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df['did_reply'].values\n",
    "true_indices = np.where(y_true)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my friends went through all of the emails, and marked which ones she thought I was likely to respond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_indices = [11, 31, 39, 51, 63, 68, 74, 78, 120, 122, 131, 133, 134, 138, 145, 159, 179]\n",
    "y_oracle = np.zeros_like(y_true)\n",
    "for i in oracle_indices:\n",
    "    y_oracle[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "She was correct for four of the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{63, 131, 133, 179}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(true_indices).intersection(oracle_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.87      0.92      0.90       172\n",
      "      reply       0.24      0.14      0.18        28\n",
      "\n",
      "avg / total       0.78      0.81      0.80       200\n",
      "\n",
      "f_2 = 0.15503875968992248\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_oracle, target_names=['no reply', 'reply']))\n",
    "print('f_2 = %s' % fbeta_score(y_true, y_oracle, 2, labels=['no reply', 'reply'], pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a machine doe better? Let's cut the dataset into a training slice (first 100 samples) and a validation slice (last 100 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice = slice(0, 100)\n",
    "test_slice = slice(100, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a feature vector based on whether the sender is somebody who we have replied to in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites = list(Counter(df.iloc[train_slice].query('did_reply == 1')['sender'].values).keys())\n",
    "len(favorites)\n",
    "\n",
    "def featureExtractor(row):\n",
    "    favorites_vector = [favorite in row['sender'] for favorite in favorites]\n",
    "    return favorites_vector\n",
    "\n",
    "def label(row):\n",
    "    return row['did_reply']\n",
    "\n",
    "X = np.array([featureExtractor(row) for idx, row in df.iterrows()])\n",
    "y_true = np.array([label(row) for idx, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning magic happens here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X[train_slice], y_true[train_slice])\n",
    "y_pred = model.predict(X[test_slice]) > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   no reply       0.90      0.95      0.93        85\n",
      "      reply       0.60      0.40      0.48        15\n",
      "\n",
      "avg / total       0.85      0.87      0.86       100\n",
      "\n",
      "f_2 = 0.42857142857142866\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true[test_slice], y_pred, target_names=[\"no reply\", \"reply\"]))\n",
    "print('f_2 = %s' % fbeta_score(y_true[test_slice], y_pred, 2, labels=['no reply', 'reply'], pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Favorites extractor from before\n",
    "def favoritesExtractor(row):\n",
    "    favorites_vector = [favorite in row['sender'] for favorite in favorites]\n",
    "    return favorites_vector\n",
    "\n",
    "#Perform rudimentary sentiment analysis on entire message body\n",
    "def sentimentExtractor(row):\n",
    "    sentiment = [TextBlob(row['body']).sentiment[0]]\n",
    "    polarity = [TextBlob(row['body']).sentiment[1]]\n",
    "    return sentiment + polarity\n",
    "\n",
    "#Determine whether there exists more than one recipient\n",
    "def recipientsExtractor(row):\n",
    "    feature_vector = [row['multiple_recipients'] == 1]\n",
    "    return feature_vector\n",
    "\n",
    "#Determine if \"Re: \" appears in subject line\n",
    "def reExtractor(row):\n",
    "    re_vector = [row['re_in_subject'] == 1]\n",
    "    return  re_vector\n",
    "\n",
    "#Determine if \"Fwd: \" appears in subject line\n",
    "def fwdExtractor(row):\n",
    "    fw_vector = [row['fw_in_subject'] == 1]\n",
    "    return fw_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([favoritesExtractor(row) \n",
    "              + recipientsExtractor(row) \n",
    "              + reExtractor(row) \n",
    "              + fwdExtractor(row) \n",
    "              + sentimentExtractor(row) \n",
    "              for idx, row in df.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             precision    recall  f1-score   support\n",
    "\n",
    "   no reply       0.77      0.79      0.78        72\n",
    "      reply       0.42      0.39      0.41        28\n",
    "\n",
    "avg / total       0.67      0.68      0.68       100\n",
    "\n",
    "f_2 = 0.398550724638 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple features, performance is about the same. Let's use a different classifier that is more suitable to the number of features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regressor\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "   no reply       0.77      0.79      0.78        72\n",
    "      reply       0.42      0.39      0.41        28\n",
    "\n",
    "avg / total       0.67      0.68      0.68       100\n",
    "\n",
    "f_2 = 0.398550724638\n",
    "\n",
    "SGDClassifier\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "   no reply       0.73      0.86      0.79        72\n",
    "      reply       0.33      0.18      0.23        28\n",
    "\n",
    "avg / total       0.62      0.67      0.63       100\n",
    "\n",
    "f_2 = 0.196850393701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance is degraded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning algorithm seems to work much better than the human."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
